/////////////////////////////////////////////////////////////////////////////////////////
//
// S T A N D A L O N E
// REVISION_TAG=1.0
/////////////////////////////////////////////////////////////////////////////////////////

//
// Maximum number of classes for which to report detailed stats in 'FINDSTATS'
// 
directory.stats.class.maxcardinality = 100

//
// Maximum number of labels for which to report detailed stats in 'FINDSTATS'
// 
directory.stats.labels.maxcardinality = 100

//
// Set to true to only run the Warp 10â„¢ Analytics Engine
//
#analytics.engine.only = true
//
// Comma separated list of headers to return in Access-Control-Allow-Headers on top of the token header
// This applies to all HTTP endpoints.
//
#cors.headers =

//
// 128 bits key for verifying class names, format is hex:hhhhh...hhhh
//
warp.hash.class = @key:128:warp.hash.class@

//
// 128 bits key for verifying labels, format is hex:hhhhh...hhhh
//
warp.hash.labels = @key:128:warp.hash.labels@

//
// 128 bits key for verifying index names, format is hex:hhhhh...hhhh
//
warp.hash.index = @key:128:warp.hash.index@

//
// 128 bits key for verifying tokens, format is hex:hhhhh...hhhh
//
warp.hash.token = @key:128:warp.hash.token@

//
// 128 bits key for verifying app names, format is hex:hhhhh...hhhh
//
warp.hash.app = @key:128:warp.hash.app@

//
// 256 bits key for protecting tokens, format is hex:hhhh...hhhh
//
warp.aes.token = @key:256:warp.aes.token@

//
// 256 bits key to generate secure scripts
//
warp.aes.scripts = @key:256:warp.aes.scripts@

//
// AES key to wrap metasets
//
warp.aes.metasets = @key:256:warp.aes.metasets@

//
// 256 bits key for protecting log messages, format is hex:hhhhh...hhhh
//
#warp.aes.logging = @key:256:warp.aes.logging@

//
// OSS Master Key, used to decrypt any 'wrapped:base64' key
//
#oss.master.key = hex:hhhhhh...

//
// Maximum number of Geo Time Series a single Plasma session can subscribe to
//
#warp.plasma.maxsubs

//
// Set to true to disable plasma
//
#warp.plasma.disable =

//
// Set to true to disable mobius
//
#warp.mobius.disable =

//
// Set to true to disable streaming updates
//
#warp.streamupdate.disable =

//
// This configuration parameter determines if undefining a function (via NULL 'XXX' DEF)
// will unshadow the original statement thus making it available again or if it will replace
// it with a function that will fail with a message saying the function is undefined.
// The safest behavior is to leave this undefined or set to 'false'.
//
warpscript.def.unshadow = false

//
// Default maximum number of operations a single WarpScript execution can do
//
warpscript.maxops = 1000
warpscript.maxops.hard = 20000

//
// Maximum number of buckets which can result of a call to BUCKETIZE
// Can be modified by MAXBUCKETS up to the hard limit below
//
warpscript.maxbuckets = 1000000
warpscript.maxbuckets.hard = 1000000

//
// Maximum number of cells in geographic shapes
// Can be modified by MAXGEOCELLS up to the hard limit below
//
warpscript.maxgeocells = 10000
warpscript.maxgeocells.hard = 100000

//
// Maximum depth of the stack
// Can be modified by MAXDEPTH up to the hard limit below
//
warpscript.maxdepth = 1000
warpscript.maxdepth.hard = 2000

//
// Maximum number of datapoint which can be fetched during a WarpScript execution
// Can be modified by LIMIT up to the hard limit below
//
warpscript.maxfetch = 100000
warpscript.maxfetch.hard = 1000000

//
// Maximum number of GTS which can be retrieved from Directory during a WarpScript execution
// Can be modified by MAXGTS up to the hard limit below
//
warpscript.maxgts = 100000
warpscript.maxgts.hard = 100000

//
// Maximum time (in ms) that can be spent in a loop
// Can be modified by MAXLOOP up to the hard limit below
//
warpscript.maxloop = 5000
warpscript.maxloop.hard = 10000

//
// Directory of Warp10 standalone install
//
standalone.home = /opt/warp10-@VERSION@

//
// Jar Repository - for Warp10 UDF only
//

warpscript.jars.directory = ${standalone.home}/jars
warpscript.jars.refresh = 60000
warpscript.jars.fromclasspath = true

//
// Macro Repository
//
warpscript.repository.directory = ${standalone.home}/macros
warpscript.repository.refresh = 60000

// Set to false to disable on demand loading of macros not yet loaded.
#warpscript.repository.ondemand = false

//
// Maximum levels of recursion in macro calls
//
warpscript.maxrecursion = 16
warpscript.maxrecursion.hard = 32

//
// Maximum number of symbols which can be defined by a single WarpScript
// Can be modified by MAXSYMBOLS up to the hard limit below
warpscript.maxsymbols = 64
warpscript.maxsymbols.hard = 256

//
// PGraphics Maximum number of pixels per image
//
warpscript.maxpixels = 1000000
warpscript.maxpixels.hard = 1000000

//
// WEBCALL limit per WarpScript
//
warpscript.maxwebcalls = 4

//
// WEBCALL user agent
//
webcall.user.agent = Warp10-WebCall

//
// List of patterns to include/exclude for hosts in WEBCALL calls
//
// Typical value is .*,!^127.0.0.1$,!^localhost$,!^192.168.*,!^10.*,!^172.(16|17|18|19|20|21|22|23|24|25|26|27|28|29|39|31)\..*
// 
//
webcall.host.patterns = !.*

//
// CALL root directory property
//
warpscript.call.directory = ${standalone.home}/calls

//
// Maximum number of subprogram instances which can be spawned by CALL
//
//warpscript.call.maxcapacity = 1

//   
// Path of the 'bootstrap' WarpScript code    
//    
//warpscript.bootstrap.path = /opt/warp10/etc/hello.mc2

//
// How often to reload the bootstrap code (in ms)
//
//warpscript.bootstrap.period = 120000

//
// Comma separated list of WarpScriptExtension classes to instantiate to modify the defined WarpScript functions.
// Extension classes can be prefixed with SORTKEY# to force the order in which the extensions will be loaded.
// The SORTKEY# prefix will be used only for sorting.
//
//warpscript.extensions = 

//
// Specific extension loading
// Extension classes can be prefixed with SORTKEY# to force the order in which the extensions will be loaded.
// The SORTKEY# prefix will be used only for sorting.
//
//warpscript.extension.xxx = package.class

//
// Specific namespace under which to load an extension. The specified namespace will be used as a prefix for all functions of the extension.
//
//warpscript.namespace.package.class = namespace.

//
// Comma separated list of additional languages to support within WarpScript. This MUST be set as a system property
//
//warpscript.languages = 

//
// Flag to enable REXEC
//
//warpscript.rexec.enable = 

//
// Time units of the platform
// ns means we store nanoseconds
// us means we store microseconds
// ms means we store milliseconds
//
warp.timeunits = us

//
// Comma separated list of Warp 10 plugins to instantiate. 
//
warp10.plugins = io.warp10.plugins.quantum.QuantumPlugin

//
// Path of the 'bootstrap' warpscript code for Egress
//
warpscript.bootstrap.path = ${standalone.home}/etc/bootstrap/egress.mc2

//
// How often to reload the bootstrap code (in ms) for Egress
//
warpscript.bootstrap.period = 120000

//
// Path of the 'bootstrap' warpscript code for Mobius
//
warpscript.mobius.bootstrap.path = ${standalone.home}/etc/bootstrap/mobius.mc2

//
// Number of threads in the Mobius pool (those threads are used to execute the mobius macros)
//
warpscript.mobius.pool = 16

//
// How often to reload the bootstrap code (in ms) for Mobius
//
warpscript.mobius.bootstrap.period = 120000

//
// Path of the 'bootstrap' warpscript code for Runner
//
warpscript.runner.bootstrap.path = ${standalone.home}/etc/bootstrap/runner.mc2

//
// How often to reload the bootstrap code (in ms) for Runner
//
warpscript.runner.bootstrap.period = 120000

//
// URL for the 'update' endpoint
//
warpscript.update.endpoint = http://${standalone.host}:${standalone.port}/api/v0/update

//
// URL for the 'meta' endpoint
//
warpscript.meta.endpoint = http://${standalone.host}:${standalone.port}/api/v0/meta

//
// URL for the 'delete' endpoint
//
warpscript.delete.endpoint = http://${standalone.host}:${standalone.port}/api/v0/delete

//
// Directory where the leveldb files should be created
//
leveldb.home = ${standalone.home}/leveldb

//
// Flag to disable the native LevelDB implementation
// This flag is false by default if the configuration is missing 
// When running WarpInit or WarpRepair, use a system property with the same name
//
leveldb.native.disable = false

//
// Flag to disable the use of the pure Java LevelDB implementation
// This flag is false by default if the configuration is missing 
// When running WarpInit or WarpRepair, use a system property with the same name
//
leveldb.java.disable = false

//
// Rate of synchronous writes for the datapoints (update/deletes).
// This is a double between 0.0 (all writes asynchronous) and 1.0 (all writes synchronous).
// The default value is 1.0 (all writes are synchronous)
//
leveldb.data.syncrate = 1.0

//
// Rate of synchronous writes for the directory writes.
// This is a double between 0.0 (all writes asynchronous) and 1.0 (all writes synchronous)
// The default value is 1.0 (all writes are synchronous)
//
leveldb.directory.syncrate = 1.0

//
// AES key to use for wrapping metadata prior to storage in leveldb
//
#leveldb.metadata.aes = 

//
// AES key to use for wrapping datapoints prior to storage in leveldb
//
#leveldb.data.aes = 

//
// @deprecated
// AES key to use for storing index details in leveldb
//
#leveldb.index.aes = 

//
// Cache size for leveldb (in bytes)
//
leveldb.cache.size = 100000000

//
// Compression type to use for leveldb (SNAPPY/NONE)
//
leveldb.compression.type = SNAPPY

//
// Maximum number of open files to use for LevelDB
//
//leveldb.maxopenfiles = 

//
// IP to bind to for listening to incoming connections. Use 0.0.0.0 to listen to all interfaces
//
standalone.host = 127.0.0.1

//
// Port to bind to for listening to incoming connections.
//
standalone.port = 8080

//
// Number of Jetty acceptors
//
standalone.acceptors = 1

//
// Idle timeout (in ms)
//
standalone.idle.timeout = 30000

//
// Number of Jetty selectors
//
standalone.selectors = 4

//
// Maximum encoder size (in bytes) for internal data transfers. Use values from 64k to 512k
//
standalone.max.encoder.size = 100000

//
// Path to a file to use for triggering compaction suspension to take snapshots
//
standalone.snapshot.trigger = ${leveldb.home}/snapshot.trigger

//
// Path to a file to use for signaling that compactions are suspended
//
standalone.snapshot.signal = ${leveldb.home}/snapshot.signal

//
// Set to true to disable the delete endpoint.
//
standalone.delete.disable = false

//
// Set the size of delete batches in number of records (defaults to 100000)
//
#standalone.max.delete.batchsize =

//
// Configuration parameter to modify the datalog header. DO NOT MODIFY
//
#http.header.datalog =

//
// Max message size for the stream update websockets
//
ingress.websocket.maxmessagesize = 1048576

//
// Max message size for the Plasma Frontend Websocket
//
plasma.frontend.websocket.maxmessagesize = 1048576

//
// Period between updates of last activity timestamps for Geo Time Series
// The value of this parameter is in ms and determines how often the directory
// will be updated when activity is being tracked.
//
#ingress.activity.window = 3600000

//
// Set to true to consider updates when tracking activity of GTS
//
#ingress.activity.update = true

//
// Set to true to consider attributes updates (calls to /meta) when tracking activity of GTS
//
#ingress.activity.meta = true

//
// Set to 'true' to indicate the instance will use memory only for storage. This type of instance is non persistent.
//
in.memory = false

//
// Depth of timestamps to retain (in ms)
//
//in.memory.depth = 

//
// High water mark in bytes. When memory goes above this threshold, attempts to remove expired datapoints will be
// done until consumed memory goes below the low water mark (see below) or no more expired datapoints can be found.
//
//in.memory.highwatermark = 

//
// Low water mark in bytes for garbage collection (see above)
//
//in.memory.lowwatermark = 

//
// If set to true, then only the last recorded value of a GTS is kept
//
//in.memory.ephemeral = 

//
// Set to 'true' to use a chunked version of the in memory datastore
// When this datastore is in effect, the depth of data retained is
// configured using in.memory.chunk.count and in.memory.chunk.length
// All other in.memory.* parameters are ignored
//
//in.memory.chunked = true

//
// Number of chunks to retain (defaults to 3)
//
//in.memory.chunk.count =

//
// Length in platform time units of each chunk (defaults to 2**63)
//
//in.memory.chunk.length =

//
// Path to a dump file containing the state of an in-memory Warp 10 to restore.
//
#in.memory.load =
  
//
// Set to true to tolerate errors while loading a dumped state. Setting this to true can lead to partial data being loaded.
//
#in.memory.load.failsafe = true

//
// Path to a dump file in which the current state of an in-memory Warp 10 will be persisted.
//
#in.memory.dump =
  
// 
// How often (in ms) to perform a gc of the in-memory store.
//
#in.memory.gcperiod =
  
//
// Maximum size (in bytes) of re-allocations performed during a gc cycle of the chunked in-memory store. 
//
#in.memory.gc.maxalloc =

//
// Set to 'true' to only forward data to Plasma. Not data storage will take place.
//
//pureplasma = 

//
// Number of threads to use for scheduling parallel scanners. Use 0 to disable parallel scanners
//
#standalone.parallelscanners.poolsize =

//
// Maximum number of parallel scanners per fetch request. Use 0 to disable parallel scanners.
//
#standalone.parallelscanners.maxinflightperrequest =

//
// Minimum number of GTS to assign to a parallel scanner. If the number of GTS to fetch is below this limit, no
// parallel scanners will be spawned. Defaults to 4.
//
#standalone.parallelscanners.min.gts.perscanner =

//
// Maximum number of parallel scanners to use when fetching datapoints for a batch of GTS (see EGRESS_FETCH_BATCHSIZE).
// Defaults to 16.
//
#standalone.parallelscanners.max.parallel.scanners =

//
// Set to true to enable splits generation to allow access to the standalone instance using Warp10InputFormat
//
#standalone.splits.enable = true

//
// Key to use for encrypting GTSSplit instances
//
#egress.fetcher.aes = 

//
// Maximum age of a valid GTSSplit (in ms)
//
#egress.fetcher.maxsplitage = 

//
// Should the egress exec handler expose its store/directory clients?
//
egress.clients.expose = false

//
// Q U A N T U M
//
/////////////////////////////////////////////////////////////////////////////////////////


//
// Quantum - Host/Port to bind to
//
quantum.port = 8090
quantum.host = ${standalone.host}


//
// D A T A L O G
//
/////////////////////////////////////////////////////////////////////////////////////////

//
// Datalogging directory. If set, every data modification action (UPDATE, META, DELETE) will produce
// a file in this directory with the timestamp, the token used and the action type. These files can then
// be used to update another instance of Warp 10
//
#datalog.dir = ${standalone.home}/datalog

//
// Unique id for this datalog instance.
//
#datalog.id = datalog-0

//
// Datalog pre-shared key to protect the DatalogRequest instances. If this AES key is not set, the requests will not be
// encrypted and could be altered.
//
#datalog.psk = hex:hhhhhhhh....

//
// Set this property to 'false' to skip logging forwarded requests or to 'true' if you want to log them to
// forward them to an additional hop.
//
#datalog.logforwarded = false

//
// Directory where datalog files to forward reside. If this property and 'datalog.forwarder.dstdir' are set, then
// the DatalogForwarder daemon will run.
//
#datalog.forwarder.srcdir = ${standalone.home}/datalog

//
// Directory where forwarded datalog files will be moved. MUST be on the same device as datalog.forwarder.srcdir
//
#datalog.forwarder.dstdir = ${standalone.home}/datalog_done

//
// Set to 'true' to delete the datalog files which were forwarded. If set to false or absent, such files will be moved
// to 'datalog.forwarder.dstdir'.
//
#datalog.forwarder.deleteforwarded = true

//
// Set to 'true' to delete the datalog files which were ignored. If set to false or absent, such files will be moved
// to 'datalog.forwarder.dstdir'.
//
#datalog.forwarder.deleteignored = true

//
// How often (in ms) to scan 'datalog.forwarder.srcdir' for datalog files to forward
//
#datalog.forwarder.period = 1000

//
// Comma separated list of datalog ids which should not be forwarded. This is used to avoid loops.
//
#datalog.forwarder.ignored =

//
// Set this property to 'true' to compress the forwarded requests
//
#datalog.forwarder.compress = true

//
// Set this property to 'true' to behave like a normal Warp 10 client. If not set to 'true' then
// the original datalog requests will be forwarded. Set to 'true' when forwarding to a distributed Warp 10.
//
#datalog.forwarder.actasclient = true

//
// Number of threads which will process the datalog requests. Requests for a identical (producer,app,owner) tuple
// will always be processed by the same thread to guarantee the sequence of actions.
//
#datalog.forwarder.nthreads = 4

//
// Endpoint to use when forwarding datalog UPDATE requests.
//
#datalog.forwarder.endpoint.update = http://host:port/api/v0/update

//
// Endpoint to use when forwarding datalog META requests.
//
#datalog.forwarder.endpoint.meta = http://host:port/api/v0/meta

//
// Endpoint to use when forwarding datalog DELETE requests.
//
#datalog.forwarder.endpoint.delete = http://host:port/api/v0/delete

//
// TRL (TOKEN REVOCATION LIST)
//
/////////////////////////////////////////////////////////////////////////////////////////

//
// root directory where trl files are stored.
//
# warp.trl.dir = ${standalone.home}/etc/trl

//
// Period (in ms) between two scans of the trl directory
//
# warp.trl.scan.period = 60000

//
// Delay (in ms) Startup delay authorized without a TRL present
// during this time tokens are decoded, even if TRL is missing.
// After this delay, if the TRL is still missing, token decoding is locked.
//
// 0 the TRL is mandatory for the token filter startup.
//
//
# warp.trl.startup.delay = 60000

//
// T H R O T T L I N G    M A N A G E R
//
/////////////////////////////////////////////////////////////////////////////////////////

//
// Name of system property (configuration property) which contains the
// root directory where throttle files are stored.
//  
throttling.manager.dir = ${standalone.home}/etc/throttle

//
// Period (in ms) between two scans of the THROTTLING_MANAGER_DIR
//
throttling.manager.period = 1000

//
// Ramp up period (in ms) during which we do not push the estimators to Sensision.
// This period (in ms) should be greater than the period at which the throttling files
// are updated, so we get a chance to have a merged estimator pushed to us even when
// we just restarted.
//
throttling.manager.rampup = 120000

//
// Default value for the rate when not configured through a file
//
throttling.manager.rate.default = 1000000.0

//
// Default value for the mads when not configured through a file
//
throttling.manager.mads.default = 100000

//
// G E O D I R
//
/////////////////////////////////////////////////////////////////////////////////////////

//
// Comma separated list of GeoDirectory instances to maintain.
// Each instance is defined by a string with the following format:
// 
// name/resolution/chunks/chunkdepth
// 
// name is the name of the GeoDirectory
// resolution is a number between 1 and 15 defining the resolution of the geo index:
// 
// 1 = 10,000 km
// 2 =  2,500 km
// 3 =    625 km
// 4 =    156 km
// 5 =     39 km
// 6 =     10 km
// 7 =  2,441 m
// 8 =    610 m
// 9 =    153 m
// 10=     38 m
// 11=     10 m
// 12=    238 cm 
// 13=     60 cm
// 14=     15 cm
// 15=      4 cm
// 
// chunks is the number of time chunks to maintain
// chunkdepth is the time span of each time chunk, in ms
//
#standalone.geodirs = 

//
// Delay in ms between two subscription updates
//
#standalone.geodir.delay = 5000

//
// Maximum number of 'cells' in the query area, system will attempt to reduce the number
// of cells searched by replacing small cells with their enclosing parent until the number
// of cells falls below this maximum or no more simplification can be done.
// 
// A good value for performance is around 256
//
#standalone.geodir.maxcells = 256

//
// AES encryption key for subscriptions
//
#standalone.geodir.aes = hex:hhhh...

//
// Directory where subscriptions should be stored
//
#standalone.geodir.subs.dir = ${standalone.home}/geodir

//
// Prefix for subscription files
//
#standalone.geodir.subs.prefix =

//
// R U N N E R
//
/////////////////////////////////////////////////////////////////////////////////////////

//
// Set to true to run each script a first time at startup, to false to schedule the
// first run at the next timestamp which is congruent to 0 modulo the period of the
// script
runner.runatstartup = true

//
// String uniquely identifying this instance of ScriptRunner
//
runner.id = runner-standalone-1

//
// Roles of the ScriptRunner instance. Can either be 'standalone' or any combination of 'scheduler' and 'worker'.
//
runner.roles = standalone

//
// warpscript endpoint to use for executing the scripts. If this is omitted then
// execution will take place in the scheduling thread.
//
//runner.endpoint = http://127.0.0.1:8881/api/v0/exec/warpscript

//
// AES key for wrapping a runner nonce which can later be extracted using RUNNERNONCE
//
//runner.psk = @key:256:runner.psk@

//
// Root directory under which scripts to run reside.
// This directory contains subdirectories, each one of them contains other
// subdirectories whose name is the periodicity (in ms) at which the scripts under it
// should be run. The scripts MUST end with the '.mc2' extension.
//
runner.root = ${standalone.home}/warpscripts

//
// Number of threads to use for running scripts.
//
runner.nthreads = 1

//
// How often (in ms) to scan RUNNER_ROOT for new scripts
//
runner.scanperiod = 60000

//
// Minimum period at which a script can be scheduled. Any script scheduled
// more often than that won't be run
//
runner.minperiod = 1000

